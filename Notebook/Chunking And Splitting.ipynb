{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 8 CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "df = pd.read_csv('Data/data.csv')\n",
    "\n",
    "# Choose the column you want to split the data by\n",
    "split_column = 'source'\n",
    "\n",
    "# Shuffle the data to ensure randomness\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Determine how many rows should go into each split\n",
    "num_splits = 8\n",
    "rows_per_split = len(df) // num_splits\n",
    "remainder = len(df) % num_splits\n",
    "\n",
    "# Create empty dataframes for each split\n",
    "splits = []\n",
    "\n",
    "start_idx = 0\n",
    "for i in range(num_splits):\n",
    "    # Add the remainder rows one by one to the first 'remainder' splits\n",
    "    end_idx = start_idx + rows_per_split + (1 if i < remainder else 0)\n",
    "    split = df.iloc[start_idx:end_idx]\n",
    "    splits.append(split)\n",
    "    start_idx = end_idx\n",
    "\n",
    "# Save each split into a CSV file\n",
    "for i, split in enumerate(splits):\n",
    "    split.to_csv(f'Data/Chunked_Data/split_{i+1}.csv', index=False)\n",
    "\n",
    "print(f\"Data has been split into {num_splits} CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk_1.csv\n",
      "Saved chunk_2.csv\n",
      "Saved chunk_3.csv\n",
      "Saved chunk_4.csv\n",
      "Saved chunk_5.csv\n",
      "Saved chunk_6.csv\n",
      "Saved chunk_7.csv\n",
      "Saved chunk_8.csv\n",
      "Saved chunk_9.csv\n",
      "Saved chunk_10.csv\n",
      "Saved chunk_11.csv\n",
      "Saved chunk_12.csv\n",
      "Saved chunk_13.csv\n",
      "Saved chunk_14.csv\n",
      "Saved chunk_15.csv\n",
      "Saved chunk_16.csv\n",
      "Saved chunk_17.csv\n",
      "Saved chunk_18.csv\n",
      "Saved chunk_19.csv\n",
      "Saved chunk_20.csv\n",
      "Saved chunk_21.csv\n",
      "Saved chunk_22.csv\n",
      "Saved chunk_23.csv\n",
      "Saved chunk_24.csv\n",
      "Saved chunk_25.csv\n",
      "Saved chunk_26.csv\n",
      "Saved chunk_27.csv\n",
      "Saved chunk_28.csv\n",
      "Saved chunk_29.csv\n",
      "Saved chunk_30.csv\n",
      "Saved chunk_31.csv\n",
      "Saved chunk_32.csv\n",
      "Saved chunk_33.csv\n",
      "Saved chunk_34.csv\n",
      "Saved chunk_35.csv\n",
      "Saved chunk_36.csv\n",
      "Saved chunk_37.csv\n",
      "Saved chunk_38.csv\n",
      "Saved chunk_39.csv\n",
      "Saved chunk_40.csv\n",
      "Saved chunk_41.csv\n",
      "Saved chunk_42.csv\n",
      "Saved chunk_43.csv\n",
      "Saved chunk_44.csv\n",
      "Saved chunk_45.csv\n",
      "Saved chunk_46.csv\n",
      "Saved chunk_47.csv\n",
      "Saved chunk_48.csv\n",
      "Saved chunk_49.csv\n",
      "Saved chunk_50.csv\n",
      "Saved chunk_51.csv\n",
      "Saved chunk_52.csv\n",
      "Saved chunk_53.csv\n",
      "Saved chunk_54.csv\n",
      "Saved chunk_55.csv\n",
      "Saved chunk_56.csv\n",
      "Saved chunk_57.csv\n",
      "Saved chunk_58.csv\n",
      "Saved chunk_59.csv\n",
      "Saved chunk_60.csv\n",
      "Saved chunk_61.csv\n",
      "Saved chunk_62.csv\n",
      "Saved chunk_63.csv\n",
      "Saved chunk_64.csv\n",
      "Saved chunk_65.csv\n",
      "Saved chunk_66.csv\n",
      "Saved chunk_67.csv\n",
      "Saved chunk_68.csv\n",
      "Saved chunk_69.csv\n",
      "Saved chunk_70.csv\n",
      "Saved chunk_71.csv\n",
      "Saved chunk_72.csv\n",
      "Saved chunk_73.csv\n",
      "Saved chunk_74.csv\n",
      "Saved chunk_75.csv\n",
      "Saved chunk_76.csv\n",
      "Saved chunk_77.csv\n",
      "Saved chunk_78.csv\n",
      "Saved chunk_79.csv\n",
      "Saved chunk_80.csv\n",
      "Saved chunk_81.csv\n",
      "Saved chunk_82.csv\n",
      "Saved chunk_83.csv\n",
      "Saved chunk_84.csv\n",
      "Saved chunk_85.csv\n",
      "Saved chunk_86.csv\n",
      "Saved chunk_87.csv\n",
      "Saved chunk_88.csv\n",
      "Saved chunk_89.csv\n",
      "Saved chunk_90.csv\n",
      "Saved chunk_91.csv\n",
      "Saved chunk_92.csv\n",
      "Saved chunk_93.csv\n",
      "Saved chunk_94.csv\n",
      "Saved chunk_95.csv\n",
      "Saved chunk_96.csv\n",
      "Saved chunk_97.csv\n",
      "Saved chunk_98.csv\n",
      "Saved chunk_99.csv\n",
      "Saved chunk_100.csv\n",
      "Saved chunk_101.csv\n",
      "Saved chunk_102.csv\n",
      "Saved chunk_103.csv\n",
      "Saved chunk_104.csv\n",
      "Saved chunk_105.csv\n",
      "Saved chunk_106.csv\n",
      "Saved chunk_107.csv\n",
      "Saved chunk_108.csv\n",
      "Saved chunk_109.csv\n",
      "Saved chunk_110.csv\n",
      "Saved chunk_111.csv\n",
      "Saved chunk_112.csv\n",
      "Saved chunk_113.csv\n",
      "Saved chunk_114.csv\n",
      "Saved chunk_115.csv\n",
      "Saved chunk_116.csv\n",
      "Saved chunk_117.csv\n",
      "Saved chunk_118.csv\n",
      "Saved chunk_119.csv\n",
      "Saved chunk_120.csv\n",
      "Saved chunk_121.csv\n",
      "Saved chunk_122.csv\n",
      "Saved chunk_123.csv\n",
      "Saved chunk_124.csv\n",
      "Saved chunk_125.csv\n",
      "Saved chunk_126.csv\n",
      "Saved chunk_127.csv\n",
      "Saved chunk_128.csv\n",
      "Saved chunk_129.csv\n",
      "Saved chunk_130.csv\n",
      "Saved chunk_131.csv\n",
      "Saved chunk_132.csv\n",
      "Saved chunk_133.csv\n",
      "Saved chunk_134.csv\n",
      "Saved chunk_135.csv\n",
      "Saved chunk_136.csv\n",
      "Saved chunk_137.csv\n",
      "Saved chunk_138.csv\n",
      "Saved chunk_139.csv\n",
      "Saved chunk_140.csv\n",
      "Saved chunk_141.csv\n",
      "Saved chunk_142.csv\n",
      "Saved chunk_143.csv\n",
      "Saved chunk_144.csv\n",
      "Saved chunk_145.csv\n",
      "Saved chunk_146.csv\n",
      "Saved chunk_147.csv\n",
      "Saved chunk_148.csv\n",
      "Saved chunk_149.csv\n",
      "Saved chunk_150.csv\n",
      "Saved chunk_151.csv\n",
      "Saved chunk_152.csv\n",
      "Saved chunk_153.csv\n",
      "Saved chunk_154.csv\n",
      "Saved chunk_155.csv\n",
      "Saved chunk_156.csv\n",
      "Saved chunk_157.csv\n",
      "Saved chunk_158.csv\n",
      "Saved chunk_159.csv\n",
      "Saved chunk_160.csv\n",
      "Saved chunk_161.csv\n",
      "Saved chunk_162.csv\n",
      "Saved chunk_163.csv\n",
      "Saved chunk_164.csv\n",
      "Saved chunk_165.csv\n",
      "Saved chunk_166.csv\n",
      "Saved chunk_167.csv\n",
      "Saved chunk_168.csv\n",
      "Saved chunk_169.csv\n",
      "Saved chunk_170.csv\n",
      "Saved chunk_171.csv\n",
      "Saved chunk_172.csv\n",
      "Saved chunk_173.csv\n",
      "Saved chunk_174.csv\n",
      "Saved chunk_175.csv\n",
      "Saved chunk_176.csv\n",
      "Saved chunk_177.csv\n",
      "Saved chunk_178.csv\n",
      "Saved chunk_179.csv\n",
      "Saved chunk_180.csv\n",
      "Saved chunk_181.csv\n",
      "Saved chunk_182.csv\n",
      "Saved chunk_183.csv\n",
      "Saved chunk_184.csv\n",
      "Saved chunk_185.csv\n",
      "Saved chunk_186.csv\n",
      "Saved chunk_187.csv\n",
      "Saved chunk_188.csv\n",
      "Saved chunk_189.csv\n",
      "Saved chunk_190.csv\n",
      "Saved chunk_191.csv\n",
      "Saved chunk_192.csv\n",
      "Saved chunk_193.csv\n",
      "Saved chunk_194.csv\n",
      "Saved chunk_195.csv\n",
      "Saved chunk_196.csv\n",
      "Saved chunk_197.csv\n",
      "Saved chunk_198.csv\n",
      "Saved chunk_199.csv\n",
      "Saved chunk_200.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = 'Data/Splitted_Data/split_1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the size of each chunk (number of rows per chunk)\n",
    "num_chunks = 200\n",
    "chunk_size = math.ceil(len(df) / num_chunks)\n",
    "\n",
    "# Loop through and save each chunk as a separate CSV file\n",
    "for i in range(num_chunks):\n",
    "    start_row = i * chunk_size\n",
    "    end_row = start_row + chunk_size\n",
    "    chunk = df.iloc[start_row:end_row]\n",
    "    \n",
    "    # Save each chunk as a new CSV file\n",
    "    chunk_file_name = f'chunk_{i+1}.csv'\n",
    "    chunk.to_csv(f'Data/Chunked_Data_1/{chunk_file_name}', index=False)\n",
    "    print(f'Saved {chunk_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/Feature_Extraction/chunk_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 484 entries, 0 to 483\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   text                     484 non-null    object \n",
      " 1   source                   484 non-null    object \n",
      " 2   prompt_id                484 non-null    int64  \n",
      " 3   text_length              484 non-null    int64  \n",
      " 4   word_count               484 non-null    int64  \n",
      " 5   label                    484 non-null    int64  \n",
      " 6   gpt2-large-text_ppl      484 non-null    float64\n",
      " 7   gpt2-large-max_sent_ppl  484 non-null    float64\n",
      " 8   gpt2-large-sent_ppl_avg  484 non-null    float64\n",
      " 9   gpt2-large-sent_ppl_std  484 non-null    float64\n",
      " 10  gpt2-large-max_step_ppl  484 non-null    float64\n",
      " 11  gpt2-large-step_ppl_avg  484 non-null    float64\n",
      " 12  gpt2-large-step_ppl_std  484 non-null    float64\n",
      " 13  gpt2-large-rank_0        484 non-null    int64  \n",
      " 14  gpt2-large-rank_10       484 non-null    int64  \n",
      " 15  gpt2-large-rank_100      484 non-null    int64  \n",
      " 16  gpt2-large-rank_1000     484 non-null    int64  \n",
      "dtypes: float64(7), int64(8), object(2)\n",
      "memory usage: 64.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
